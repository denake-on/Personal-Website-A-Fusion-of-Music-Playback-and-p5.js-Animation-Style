# 大模型进化算法在符号认知模型中的应用
这段时间的主要任务就是看论文，本来一周就能看完两篇的，但是中途去了一趟舟山，第二周看完了，然后第三周约了老师，周一问的但是老师一直不回复我的消息，然后又拖了一周。

这次主要是看了**Discovering Symbolic Cognitive Models from Human and Animal Behavior**和关于FunSearch的**Mathematical Discoveries from program search with large language models.**


老师做的用户建模其实跟我想象中的有些差异，本来以为是像实习那种只要收集数据，然后在固定的几个维度做简单的分类，然后再推送对应的产品。

现在看得两篇论文更偏向于工程方面，即使用大语言模型编写代码，不断迭代找到最符合人类行为模式的一种，然后再想办法去解释这样的模型。

令我比较惊讶的主要是两个点，**一是这些策略迭代的底层逻辑其实非常简单，却有非常好的效果。二是这些模型的可解释性，有一些人类的行为模式在手工构建模型的时候其实是很难想到的，但是在被构建出来时候发现这些东西符合某种行为、在此基础上进一步解释这种行为是较为简单的。**


### 将复杂的模型选择转化为简单的 prompt engineering

首先，无论是 Symbolic Cognitive Models（使用 CogFunSerch 构建）还是 Function Search，他们都**基于一种数据驱动的模型方法，就是让算法自动探索一个很大的模型空间，然后用数据的拟合质量选择合适的模型**。

这样做的原因主要来源于两点：

一，当我们尝试用一种符号模型来描述人的认知行为时，可以选择的形式和参数组合非常多，因此很难保证研究者靠经验或者直觉选择的模型合适。

```
A key limitation of this approach is that the space of symbolic models is vast, and it is far from certain that the best possible model for a dataset is one that researchers will have considered

————《Discovering Symbolic Cognitive Models from Human and Animal Behavior》
```

二，其次是常用的Symbolic Cognitive Models大多数结构比较固定的，模型的假设也比较简单，而人类的行为往往是复杂多变的，经典模型的预测不够准确，并且解释性也不足，因此不能继续依赖这些手工构建的经典模型。

```
Indeed, recent work has used comparisons to flexible recurrent neural networks (RNNs; Dezfouli et al. 2019b; Eckstein et al. 2024; Ger et al. 2023; Song et al. 2021) to show that commonly-used symbolic models tend to dramat ically underfit their datasets.
————《Discovering Symbolic Cognitive Models from Human and Animal Behavior》
```

基于此，两篇文章主要讨论的就是，如何让算法去自动探索这样一个“很大的“模型空间呢？FunSearch提供了一种进化算法，即LLM-guided evolutionary algorithms, 这种进化算法主要有两个方面，一是”searching for neural network architecture“，即使用LLM帮助搜索最有网络层数，连接方式，以及激活函数等等。另一方面在于生成候选解的时候，用LLM提出智能建议，当使用变异/交叉生成新个体进行重复和迭代的时候，使用LLM修改或组合。

```
To surpass the “nominal” capabilities of LLMs, recent works [3] have combined them with evolutionary algorithms [15, 16], leading to important improvements on diverse synthetic problems [17], searching for neural network architectures [18–20], and solving puzzles [21].

———— 《Mathematical Discoveries from program search with large language models》
```

用更简单的话来说就是，利用大模型的力量，把一个复杂的选择模型的过程简化为Prompt engineering的过程。

FunSearch中的进化算法示例图如下，主要集中在几个关键概念和步骤中，首先是基本骨架确定，而只进化代码中的关键逻辑部分，其次是采用了一种“岛屿进化法”，让更多的程序同时进化，以及异步拓展。

```
Key to the success of this simple procedure is a combination of multiple essential ingredients. First, we sample best performing programs and feed them back into prompts for the LLM to improve on; we refer to this as best-shot prompting. Second, we start with a program in the form of a skeleton (containing boilerplate code and potentially prior structure about the problem), and only evolve the part governing the critical program logic. For example, by setting a greedy program skeleton, we evolve a priority function used to make decisions at every step. Third, we maintain a large pool of diverse programs by using an island-based evolutionary method that encourages exploration and avoids local optima. Finally, leveraging the highly parallel nature of FunSearch, we scale it asynchronously, considerably broad ening the scope of this approach to find new results, while keeping the overall cost of experiments low.

———— 《Mathematical Discoveries from program search with large language models》
```

![FunSearch 进化算法示意](./assets/media/images/Pasted%20image%2020251105144655.png)

对于这个架构，首先是Specification，这部分以骨架的形式编写，而进化的关键部分交给LLM，例如greedy算法中，其他的骨架部分写好，而给所有元素计算分数并排序的关键函数priority()则交给LLM进化。这部分设计能提高整体效果的原因在于它使得LLM能够将主要的计算资源集中在进化的关键部分上。

其次是进化的主要部分，**prompt的编写**与**岛屿进化**。
prompt的编写采用一种拼接的方式，先从程序库中选择K个程序作为父程序，将这些程序（v0,v1,v2等等）均写入prompt，再在prompt的末尾加上需要新写的一个函数头（v3:#基于v0,v1,v2进行改进）,最后将这个prompt喂给预训练过的大模型进行交叉和变异，最终形成一个新的程序。

对于岛屿进化算法，则是定期对岛屿进行排名，并且删除排名在最后一半的岛屿中的全部程序，然后从幸存的岛屿中克隆最佳程序给排名靠后的岛屿，以此保证进化的高效和信息流通。
```
Crucially, we let information flow between the islands by periodically discarding the programs in the worst half of the islands (corresponding to the ones whose best individuals have the lowest scores). We replace the programs in those islands with a new population, initialized by cloning one of the best individuals from the surviving islands.

—————— 《《Mathematical Discoveries from program search with large language models》》
```

CogFunSearch使用一种类似的方法，但是多添加了一个优化层
![CogFunSearch 框架](./assets/media/images/Pasted%20image%2020251105151015.png)
- **外部循环（A）**：在这个阶段，FunSearch 生成可能适合建模行为的程序。它通过利用 LLM 的生成能力并通过时间演化程序，开始发现符号认知模型。
    
- **内部循环（B 和 C）**：生成的程序一旦完成，就会进入第二阶段，在这个阶段，程序的参数会拟合到实际的行为数据（B）。然后，通过模型在数据上的预测与实际数据的匹配程度进行评分（C）。这个双层优化过程确保了发现的程序既具备预测能力，又具备可解释性，从而能够识别出新的有效认知模型。

以上即为FunSearch方法如何将模型选择的问题转换为prompt构建。
一开始，我以为这个方法比较依赖于模型的回答质量，在FunSearch的一开头它就有写道，“大模型倾向于编造。以及最终无法超越现有的成果”，但是FunSearch最终拥有了”得到新最优解“的成果，所以是什么部分让它得到这样的成果呢，最后部分文章又提到，其实这种方法对于LLM的确切选择其实是不敏感，他们在研究过程中也没有选择一些慢速高质量的模型，而是选择了一些比较快的模型。因此，这种方法的主要效果来源于他们设计的进化算法。（？？）


### 简单与可解释性兼得的CogFunSearch
CogFunSearch这篇文章中提到了很多有意思的模型，行为建模特别的一点在于当一个模型出现特别的参数计算的时候，我们容易去解释它，将参数套入人的行为。而手工构建模型的时候想到这种行为进行建模却是很难的。

例如大鼠数据集的baseline模型**RHG**，r是奖励，人会倾向于选择能够获得更高奖励的行为，h是习惯，尽管某些行为无法获得更高的奖励但因为我们的习惯还是会选择它，g是赌徒谬论，当一个行为很久没有被选择的时候，我们会认为它该出现了，然后选择它。

感觉有意思是因为确实发现这样的模型解释确实能够很大程度上适配我的行为，某些烟不是最好抽的但由于习惯抽所以我还是会选它；有更近更快的路但我喜欢走我更熟悉的路；蒙最后一个选择题的时候喜欢盲狙看那个选项出现的少然后选他它。在计算选择概率的时候这篇文章的模型给概率设定了一个上限，无论softmax得到的结果多么接近于1都不可能等于1

背后的逻辑在于无论一个选项能够获得多高的奖励人选择它的概率也不是百分百，因为人可能会点错，就好像明明知道答案却手滑的选择题，或者想要探索其他的选项。


又比如，在计算人类的选择概率的时候 CogFunSearch 给概率设定了一个上限，无论 softmax 得到的结果多么接近于 1 都不可能等于 1。

**背后的逻辑在于无论一个选项能够获得多高的奖励人选择它的概率也不是百分百，因为人可能会点错，就好像明明知道答案却手滑的选择题，或者想要探索其他的选项。**

其次是Q值得到平均化。传统强化学习只有被选中的动作的q值会改变，这篇文章里所有动作的q值都会因为选择了某个动作而被拉向一种平均。

**对于这个选择的解释是，人选择的时候不会让某个价值被拉得太高，而是让所有选项回归差不多好，就好像当所有选择都差不多的时候人不会固定选一个，而当一个动作长期没有被选择的时候人类则会忘记它的差。**

一些【非理性】的行为，都能够用合理的公式建模出来。





